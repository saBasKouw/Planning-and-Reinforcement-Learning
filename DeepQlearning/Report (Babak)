13.
- Do you see any other way to increase the sample-efficiency of Q-learning? 
        a. Efficient exploration of the environment. Using E-greedy exploration methods may introduce difficulties in policy updating. 
          Instead, parameter noise proved more efficient than action noise (Plappert et al., 2018) as an exploration method.
        b. By learning an environment model, if the environment is simple enough (for instance, approachable by a linear model 
          instead of a nonlinear one).
        c. Avoiding learning from scratch, and instead using existing (suboptimal) policies as a starting point. 
        d. Efficient exploration by random starting points (richer sampling).
        d. Better abstraction of actions and states
        
- Why is sample efficiency so important in real-world applications of reinforcement learning?
        a. The rich dimensionality of our natural environment results in the necessity of a much larger sample number for learning; 
          Due to the fact that there is a seemingly infinite number of possible states, and a learning agent usually develops 
          by knowledge of combinations of states and actions (and possibly reward and new states), it may take a very long time 
          to sample certain situations and thus learn from them. Without any additional measures, it could therefore take an agent 
          very long to learn in our natural environment. Creating workarounds that allows the agent to learn on less samples 
          would then be very desirable.
        
- If you want to implement your idea for additional points (max 2.5), discuss with teacher first


        
