13.
Do you see any other way to increase the sample-efficiency of Q-learning? 
        - Efficient exploration of the environment. Using E-greedy exploration methods may introduce difficulties in policy updating. 
          Instead, parameter noise proved more efficient than action noise (Plappert et al., 2018) as an exploration method. This gives
          for richer behaviour and more stable policy.
        - By learning an environment model, if the environment is simple enough (for instance, approachable by a linear model 
          instead of a nonlinear one).
        - Avoiding learning from scratch, and instead using existing (suboptimal) policies as a starting point. 
        - Efficient exploration by random starting points (richer sampling).
        - Better abstraction of actions and states. The lower the number of possible actions and states (without trading completeness
          of the representation of the environment, and avoiding over-reduction), the less samples required to learn on.
        
Why is sample efficiency so important in real-world applications of reinforcement learning?
        The rich dimensionality of our natural environment results in the necessity of a much larger sample number for learning: 
        Due to the fact that there is a seemingly infinite number of possible states, and a learning agent usually develops 
        by knowledge of combinations of states and actions (and possibly reward and new states), it may take a very long time 
        to sample certain situations and thus learn from them. Without any additional measures, it could therefore take an agent 
        very long to learn in our natural environment. Creating workarounds that allows the agent to learn on less samples 
        would then be very desirable.
        
If you want to implement your idea for additional points (max 2.5), discuss with teacher first
        Introduction of parameter space noise: Action space is not sampled for every action, rather noise is added 
        to the parameters of the policy for every episode. This addition to the basic Q-learning implementation
        is especially useful when the reward is sparse (Plappert et al., 2018). This somewhat applies to the current problem.


        
